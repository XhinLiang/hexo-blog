title: 后端基本素养 - 直播答题背后的思考
date: 2018-4-1 14:26:29
tags: [后端基本素养,随笔]
categories: 生活
toc: true
---

2017 年年底，以 “冲顶大会” “百万英雄” 为首的直播答题应用开始兴起。在 “直播答题” 这个活动的背后，后端程序员付出的努力可能超乎你想象。

##### CDN 调度
首先第一个调度的难点是直播拉流 CDN 的调度。
可能大家都发现了，这些直播答题活动的画质都很渣，为什么呢？
假设一场直播答题有 500w 用户同时在线（我不知道这两个活动的具体的真实同时在线人数，但是我估计跟这个数应该在同一个数量级），那么就会有 800w 个客户端在同时拉流。如果拉流的码率是 3200Kbps (3,200,000 bit per second)，那么对于每个用户需要的下行带宽就是 3.2 Mbps，如果都落到同一家 CDN 上，那么这家 CDN 所需要的总的上行带宽就是 3.2 * 500,000,000 Mbps。

对于不是直播的业内的开发者来说，这个概念可能不够清晰。但是我在这里立一个 Flag：*目前（**2018 Q1**）在国内，绝对没有一家 CDN 能够完全满足这样一个量级的需求。*

所以，为了扛住这样一个恐怖的量级的拉流需求，需要在各种地方做所谓的 `hack` 。

1. 跟多个 CDN 合作，各个 CDN 的流量配比动态分配。（很多 CDN 的计费是梯度的，所以为了节约开支，这个流量配比必须是可调的）
2. 最大程度地进行视频的压缩，不管他是有损压缩还是无损压缩，只要能节约带宽，统统上。为了节约成本，甚至会出现一些自定义的编码格式（服务端 + 客户端）。（这个时候，公司里管音视频技术的人就出马了）
3. 网络上的优化，机房专线，运营商优化，等等。

##### 超高并发点

直播答题起源于直播，但是需要考虑比直播更多的事情，会涉及到一些并发点。

首先是题目的下发和答题请求的处理。
题目的下发本身不是一件难事，因为所有的用户拿到的都是同样一份题目（伪造用户除外）。所以这个时候使用一个 **长连接服务器** 能够很好地完成，现在各个语言基于 `Epoll/Kqueue` 的多路复用网络库基本都能实现单机 100w+ 的同时在线了。考虑到业务的复杂性，我们假设每台机器有 20w 的同时在线。

但是题目下发之后，就要考虑到所有用户同时答题这个并发点。

这个点有多恐怖呢？假设所有用户都同时收到题目（考虑到网络延迟，时钟误差，事实上这是不可能的），然后收到题目之后都要求用户在 3 秒内作答。
大部分的用户会在 2 秒左右提交答案。假设这部分用户占 40%，那么就是 200w，那这个时候服务器需要扛的并发量就是 200w QPS。

直播答题的提交答案和直播的评论是不一样的。
直播评论很多，但是重要性很低，而且丢弃一些直播的评论对业务并没有什么影响，即使是 1000 同时在线的直播房间，评论也显示不完。所以很多情况下，直播的评论应该也必须丢弃。
但是直播答题的提交答案是不一样的，这个是业务需求，不能丢。

200w QPS相信大家都能清楚是什么概念了。
如果提交答案走的是长连接的上行通道，那么一个长连接服务器就需要扛 20w QPS 的上行请求（一个 TCP 数据包，我们姑且把它称为请求）。恕我直言，这是不可能的。
正因为提交答案不能走长连接的上行通道，所以还是要使用别的方式。

最容易想到的就是 HTTP 了。HTTP 可以有各种各样的负载均衡机制，包括硬件层面的和软件层面的。但即便如此，200w QPS 也需要很高的成本。
那么这个时候应该怎么办呢，最容易想到的无非是时间随机打散了。
考虑到用户的网络状况不同，假设 99% 的用户在 2s 内收到题目，我们在客户端层面进行时间的随机打散 -- 随机延迟 0 ~ 5s 才把题目呈现给用户。那么这个时候的峰值 QPS 就会降低到 30w 左右了。

基于 HTTP 的各种负载均衡解决方案， 30w QPS 所需要的成本已经可以接受了。
